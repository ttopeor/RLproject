
I0531 21:37:19.021018 140516767299328 _logging.py:76] Websocket connected
Environment Ready!!
CustomEnv Environment initialized
For debug - env created
For debug - ds created
/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/gym/spaces/box.py:73: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  logger.warn(
I0531 21:37:21.450048 140518564247104 xla_bridge.py:455] Unable to initialize backend 'rocm': NOT_FOUND: Could not find registered platform with name: "rocm". Available platform names are: CUDA Interpreter Host
I0531 21:37:21.451139 140518564247104 xla_bridge.py:455] Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'
I0531 21:37:21.452450 140518564247104 xla_bridge.py:455] Unable to initialize backend 'plugin': xla_extension has no attributes named get_plugin_device_client. Compile TensorFlow with //tensorflow/compiler/xla/python:enable_plugin_device set to true (defaults to false) to enable this.
For debug - agent created
For debug - replay_buffer created


100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:11<00:00,  1.10s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                   | 5/11 [00:01<00:01,  4.32it/s]
For debug - random action
For debug - action [-0.26015479  0.7696304   0.16351692] <class 'numpy.ndarray'>
For debug - random action
For debug - action [ 0.15847387 -0.89550887 -0.16246094] <class 'numpy.ndarray'>
For debug - random action
For debug - action [-0.91033936  0.1065242  -0.77206318] <class 'numpy.ndarray'>
For debug - random action
For debug - action [ 0.6842601  -0.40193363 -0.72245836] <class 'numpy.ndarray'>
For debug - random action
For debug - action [ 0.49348597 -0.06659533 -0.28320656] <class 'numpy.ndarray'>
For debug - observation [  0.14317828   0.15633905 139.80353     10.          10.        ] (5,) <class 'numpy.ndarray'>
For debug - self.rng = [   7409478 1215067383]
For debug - self.actor.apply_fn = <bound method Module.apply of Normal(
    # attributes
    base_cls = functools.partial(<class 'rlproject.networks.mlp.MLP'>, hidden_dims=(256, 256), activate_final=True, use_pnorm=False)
    action_dim = 3
    log_std_min = -20
    log_std_max = 2
    state_dependent_std = True
    squash_tanh = True
)>
For debug - self.actor.params = FrozenDict({
    MLP_0: {
        Dense_0: {
            bias: Array([-0.00125623,  0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627, -0.00125627,  0.00125627, -0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                    0.00125626, -0.00125627,  0.00125627,  0.00125627,  0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627, -0.00125627,  0.00125627,  0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627,  0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627,  0.00125626, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627,  0.00125627,  0.00125627,  0.00125627,  0.00125627,
                   -0.00125627,  0.00125627, -0.00125627,  0.00125627,  0.00125627,
                   -0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627,  0.00125627,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627,  0.00125627, -0.00125627,  0.00125627,  0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.00125627, -0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627,  0.00125627,  0.00125627,  0.00125627,
                    0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                    0.00125627, -0.00125627, -0.00125627,  0.00125627,  0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                    0.00125627,  0.00125627, -0.00125627, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125627,  0.00125627,  0.00125627,  0.00125627,
                    0.00125627,  0.00125627, -0.00125627,  0.00125627,  0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627, -0.00125625,
                   -0.00125627, -0.00125627, -0.00125627,  0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                    0.00125627,  0.00125627,  0.00125625,  0.00125627,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627,  0.00125627,  0.00125627, -0.00125627,  0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125626,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125627, -0.00125624,  0.00125627, -0.00125627,
                    0.00125627, -0.00125627,  0.00125627,  0.00125627,  0.00125627,
                   -0.00125627], dtype=float32),
            kernel: Array([[ 0.11290967,  0.11298959,  0.09699896, ...,  0.14481679,
                    -0.03509335,  0.0356598 ],
                   [ 0.0027189 ,  0.0403089 , -0.07158597, ...,  0.04622442,
                    -0.04140886, -0.06705633],
                   [-0.0066095 ,  0.0246138 , -0.06023255, ..., -0.07694968,
                    -0.15251094, -0.08909327],
                   [        nan,         nan,         nan, ...,         nan,
                            nan,         nan],
                   [        nan,         nan,         nan, ...,         nan,
                            nan,         nan]], dtype=float32),
        },
        Dense_1: {
            bias: Array([-0.00125627,  0.00125622,  0.        ,  0.00125627, -0.00125627,
                   -0.00125627, -0.00125627,  0.        , -0.00125625, -0.00125627,
                   -0.00125627, -0.00125627,  0.00125626, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125625,  0.        ,  0.        , -0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627,  0.        ,
                    0.00125627, -0.00125627,  0.00125626,  0.00125627, -0.00125627,
                   -0.00125627,  0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.00125627, -0.00125626,  0.00125627, -0.00125627,  0.00125627,
                    0.00125627,  0.        , -0.00125627, -0.00125627, -0.00125627,
                   -0.00125627,  0.00125627,  0.00125627,  0.00125627, -0.00125598,
                    0.00125627, -0.00125627,  0.00125627,  0.00125627,  0.00125627,
                    0.00125626, -0.00125626,  0.00125627, -0.00125627, -0.00125627,
                    0.        , -0.00125627, -0.00125627,  0.00125622,  0.00122011,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                   -0.00125627,  0.00125627, -0.00125627,  0.00125627, -0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125627,  0.00125626, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125624,  0.00125627,  0.        , -0.00125627,
                    0.00125623,  0.00125626, -0.00125627,  0.        ,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627,  0.00125627, -0.00125627,
                    0.00125627,  0.00125627, -0.00125627,  0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.00125627,  0.00125627, -0.00125626,
                    0.00125625, -0.00125624, -0.00125627, -0.00125627,  0.00125625,
                   -0.00125627,  0.00125627,  0.        , -0.00125621,  0.00125622,
                    0.00125627,  0.00125627,  0.        ,  0.        ,  0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627, -0.00125627,
                    0.        ,  0.00125627, -0.00125625, -0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.00125627,  0.00125622, -0.00125625,
                    0.        ,  0.        ,  0.00125627,  0.00125627, -0.00125627,
                    0.00125627,  0.00125627, -0.00125627,  0.00125627,  0.00125627,
                   -0.00125627,  0.00125627,  0.00125627, -0.00125627, -0.00125627,
                   -0.00125624,  0.00125627, -0.00125627,  0.        ,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                    0.00125627,  0.        ,  0.00125627,  0.00125627, -0.00125627,
                   -0.00125627, -0.00125625, -0.00125627,  0.00125627,  0.00125626,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.00125625,  0.00125627, -0.00125627,
                   -0.00125626, -0.00125622, -0.00125627,  0.00125627, -0.00125627,
                    0.00125627,  0.00125627,  0.        , -0.00125627, -0.00125627,
                    0.00125627, -0.00125627, -0.00125627,  0.        , -0.00125627,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627,  0.00125627,  0.00125627,
                   -0.00125626,  0.00125627,  0.00125627,  0.00125627, -0.00125627,
                    0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                   -0.00125611, -0.00125627, -0.00125627,  0.00125627, -0.00125627,
                    0.00125627, -0.00125622,  0.00125627, -0.00125627,  0.00125627,
                    0.00125627,  0.00125627, -0.00125626, -0.00125627, -0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.00125627,
                   -0.00125627, -0.00125627, -0.00125627, -0.00125627,  0.        ,
                   -0.00125627, -0.00125627,  0.00125627, -0.00125627, -0.00125627,
                    0.00125627], dtype=float32),
            kernel: Array([[nan, nan, nan, ..., nan, nan, nan],
                   [nan, nan, nan, ..., nan, nan, nan],
                   [nan, nan, nan, ..., nan, nan, nan],
                   ...,
                   [nan, nan, nan, ..., nan, nan, nan],
                   [nan, nan, nan, ..., nan, nan, nan],
                   [nan, nan, nan, ..., nan, nan, nan]], dtype=float32),
        },
    },
    OutputDenseLogStd: {
        bias: Array([nan, nan, nan], dtype=float32),
        kernel: Array([[nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan]], dtype=float32),
    },
    OutputDenseMean: {
        bias: Array([nan, nan, nan], dtype=float32),
        kernel: Array([[nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan],
               [nan, nan, nan]], dtype=float32),
    },
})
For debug - observations = [  0.14317828   0.15633905 139.80353     10.          10.        ]
For debug - key = Traced<ShapedArray(uint32[2])>with<DynamicJaxprTrace(level=1/0)>
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                   | 5/11 [00:01<00:02,  2.55it/s]
Traceback (most recent call last):
  File "/home/howard/RLproject/train_finetuning.py", line 274, in <module>
    app.run(main)
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/howard/RLproject/train_finetuning.py", line 194, in main
    next_observation, reward, done, info = env.step(action)
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/gym/wrappers/record_episode_statistics.py", line 26, in step
    observations, rewards, dones, infos = super(RecordEpisodeStatistics, self).step(
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/gym/core.py", line 289, in step
    return self.env.step(action)
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/gym/core.py", line 349, in step
    return self.env.step(self.action(action))
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/gym/core.py", line 349, in step
    return self.env.step(self.action(action))
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/site-packages/gym/wrappers/rescale_action.py", line 37, in action
    assert np.all(np.greater_equal(action, self.min_action)), (
AssertionError: (array([nan, nan, nan]), array([-1., -1., -1.]))
For debug - dist = tfp.distributions.TanhTransformedDistribution("tanhMultivariateNormalDiag", batch_shape=[], event_shape=[3], dtype=float32)
For debug - sample action
For debug - action [nan nan nan] <class 'numpy.ndarray'>
Exception ignored in: <module 'threading' from '/home/howard/anaconda3/envs/rlpd/lib/python3.9/threading.py'>
Traceback (most recent call last):
  File "/home/howard/anaconda3/envs/rlpd/lib/python3.9/threading.py", line 1477, in _shutdown
    lock.acquire()
KeyboardInterrupt: