"""This piece of code structure is generated by ChatGPT"""

import gym
from gym import spaces
import numpy as np
from pynput.keyboard import Key, Controller
import cv2
from stable_baselines3 import A2C


# Define the environment class
class PhysicalEnv(gym.Env):
    def __init__(self):
        #initialize the keyboard controller
        self.keyboard = Controller()

        self.prev_distance = None        

        # Define the observation space as a 1D array of 100x100x3 RGB images
        self.observation_space = spaces.Box(low=0, high=255, shape=(100, 100, 3), dtype=np.uint8)
        
        # Define the action space as 6 discrete commands
        self.action_space = spaces.Discrete(6)

        # Initialize the robot state
        self.robot_state = np.zeros((100, 100, 3), dtype=np.uint8)
        
        # Initialize the validation camera state
        self.validation_camera_state = np.zeros((100, 100, 3), dtype=np.uint8)
        
        # Define other necessary variables
        
    def reset(self):
        # Reset the robot to its initial state and return the initial observation
        self.robot_state = np.zeros((100, 100, 3), dtype=np.uint8)
        self.validation_camera_state = np.zeros((100, 100, 3), dtype=np.uint8)
        return self.robot_state
    
    def step(self, action):
        # Implement the logic for translating and rotating the robot
        # based on the given action
        if action == 0: # x trans positive
            self.keyboard.press('w')
            self.keyboard.release('w')
        elif action == 1: # x trans negative
            self.keyboard.press('s')
            self.keyboard.release('s')
        elif action == 2: # y trans positive
            self.keyboard.press('d')
            self.keyboard.release('d')
        elif action == 3: # y trans negative
            self.keyboard.press('a')
            self.keyboard.release('a')
        elif action == 4: # z rot positive
            self.keyboard.press('e')
            self.keyboard.release('e')
        elif action == 5: # z rot negative
            self.keyboard.press('q')
            self.keyboard.release('q')
        else:
            raise ValueError("Invalid action")
        

        # Update the robot state based on the new position and orientation
        self.robot_state = self.get_camera_state()
        
        # Calculate the reward based on the new state
        reward = self.calculate_reward()
        
        # Check if the episode is done based on the new state
        done = self.check_if_done()
        
        # Update the validation camera state
        self.validation_camera_state = self.get_validation_camera_state()
        
        # Return the new state, reward, done flag, and additional info
        return self.robot_state, reward, done, {}
        
    def get_camera_state(self):
        # Get the current RGB image from the first view camera and 
        # use it as the robot state
        rgb_image = self.get_rgb_image_from_camera()
        return rgb_image
    

    def get_validation_camera_state(self):
        # Initialize the USB camera for the third view
        cap = cv2.VideoCapture(1)

        # Check if the camera is opened successfully
        if not cap.isOpened():
            raise ValueError("Could not open camera")

        # Read in a frame from the camera
        ret, frame = cap.read()

        # Release the camera
        cap.release()

        # Resize the frame to 100x100 and convert it to RGB format
        rgb_image = cv2.cvtColor(cv2.resize(frame, (100, 100)), cv2.COLOR_BGR2RGB)

        return rgb_image


    def calculate_reward(self):
        # Get the validation camera state to detect the cube
        validation_state = self.get_validation_camera_state()

        # Define the ArUco dictionary and parameters
        aruco_dict = cv2.aruco.Dictionary_get(cv2.aruco.DICT_6X6_250)
        aruco_params = cv2.aruco.DetectorParameters_create()

        # Detect the ArUco markers in the validation state
        corners, ids, rejected = cv2.aruco.detectMarkers(validation_state, aruco_dict, parameters=aruco_params)

        # If no markers are detected, return a negative reward
        if ids is None:
            return -1

        # Get the pose estimation of the cube using the detected markers
        rvecs, tvecs, _ = cv2.aruco.estimatePoseSingleMarkers(corners, 0.05, self.camera_matrix, self.dist_coeffs)

        # If the pose estimation is not successful, return a negative reward
        if rvecs is None or tvecs is None:
            return -1

        # Get the rotation matrix from the rotation vector
        R, _ = cv2.Rodrigues(rvecs)

        # Define the cube vertices in the object coordinate system
        cube_vertices = np.array([
            [-0.025, -0.025, -0.025],
            [-0.025, -0.025, 0.025],
            [-0.025, 0.025, 0.025],
            [-0.025, 0.025, -0.025],
            [0.025, -0.025, -0.025],
            [0.025, -0.025, 0.025],
            [0.025, 0.025, 0.025],
            [0.025, 0.025, -0.025]
        ])

        # Transform the cube vertices to the camera coordinate system
        cube_vertices_cam = np.dot(R, cube_vertices.T).T + tvecs

        # Project the cube vertices onto the image plane
        cube_vertices_img, _ = cv2.projectPoints(cube_vertices_cam, rvecs, tvecs, self.camera_matrix, self.dist_coeffs)
        cube_vertices_img = np.int32(cube_vertices_img).reshape(-1, 2)

        # Define the frame color range in HSV format
        lower_color = np.array([0, 0, 0])
        upper_color = np.array([180, 255, 50])

        # Convert the validation state to HSV format
        hsv = cv2.cvtColor(validation_state, cv2.COLOR_RGB2HSV)

        # Threshold the image based on the frame color range
        mask = cv2.inRange(hsv, lower_color, upper_color)

        # Find the contours in the thresholded image
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        # If no contours are found, return a negative reward
        if len(contours) == 0:
            return -1

        # Find the largest contour, which should be the frame
        frame_contour = max(contours, key=cv2.contourArea)

        # Draw the frame contour on the validation state for visualization
        cv2.drawContours(validation_state, [frame_contour], -1, (0, 255, 0), 2)

        # Define the frame as a polygon using the frame contour
        frame_poly = np.array([frame_contour[:, 0]])

        # Define the cube as a polygon using the cube vertices
        cube_poly = np.array([cube_vertices_img[0], cube_vertices_img[1], cube_vertices_img[2], cube_vertices_img[3], cube_vertices_img[0]])

        # Check if the cube is within the frame
        if cv2.pointPolygonTest(frame_poly, tuple(cube_poly[0]), False) >= 0 and \
        cv2.pointPolygonTest(frame_poly, tuple(cube_poly[1]), False) >= 0 and \
        cv2.pointPolygonTest(frame_poly, tuple(cube_poly[2]), False) >= 0 and \
        cv2.pointPolygonTest(frame_poly, tuple(cube_poly[3]), False) >= 0:
            # If the cube is within the frame, calculate the distance between the centers of the cube and the frame
            frame_center = np.array([np.mean(frame_poly[:, 0])])
            cube_center = np.array([np.mean(cube_poly[:, 0]), np.mean(cube_poly[:, 1])])
            distance = np.linalg.norm(frame_center - cube_center)
            # Calculate the maximum possible distance between the centers of the cube and the frame
            max_distance = np.linalg.norm(np.array([self.width, self.height]))

            # Calculate the reward based on the distance between the centers of the cube and the frame
            reward = 1 - (distance / max_distance)

            # Check if the distance has increased compared to the previous time step
            if self.prev_distance is not None and distance > self.prev_distance:
                # If the distance has increased, give a negative reward
                reward -= 0.1

            # Store the current distance for the next time step
            self.prev_distance = distance

            # Return the reward
            return reward
        else:
            # If the cube is outside the frame, return a negative reward
            return -1
        
    
    def check_if_done(self):
        # Calculate the reward to check if the cube is within the frame and if it is stable
        reward = self.calculate_reward()

        # If the reward is negative, the cube is not within the frame or it is not stable, and the task is not done
        if reward < 0:
            return False

        # If the distance between the centers of the cube and the frame has increased, the task is not done
        if self.prev_distance is not None and distance > self.prev_distance:
            return False

        # If the cube is within the frame, it is stable, and the distance between the centers of the cube and the frame has not increased, the task is done
        return True
    
    def get_rgb_image_from_camera(self):
        # Implement the logic for capturing the current RGB image from the first view camera
        # and returning it as a 100x100x3 numpy array
        # Initialize the USB camera
        cap = cv2.VideoCapture(0)

        # Check if the camera is opened successfully
        if not cap.isOpened():
            raise ValueError("Could not open camera")

        # Read in a frame from the camera
        ret, frame = cap.read()

        # Release the camera
        cap.release()

        # Resize the frame to 100x100 and convert it to RGB format
        rgb_image = cv2.cvtColor(cv2.resize(frame, (100, 100)), cv2.COLOR_BGR2RGB)

        return rgb_image

if __name__ == '__main__':
    # Create the PhysicalEnv environment
    env = PhysicalEnv()

    # Set the random seed for reproducibility
    np.random.seed(42)
    env.seed(42)

    # Create the A2C agent
    model = A2C('MlpPolicy', env, verbose=1)

    # Train the agent for 10 trials
    for i in range(10):
        # Reset the environment
        obs = env.reset()

        # Train the agent for one episode
        done = False
        while not done:
            action, _ = model.predict(obs)
            obs, reward, done, info = env.step(action)
            model.learn(total_timesteps=1)

        # Print the episode reward
        print(f'Trial {i+1} reward: {info["episode"]["r"]}')

        # Save the trained model
        model.save(f'a2c_physicalenv_trial_{i}')

    # Close the environment
    env.close()
